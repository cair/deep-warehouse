{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "source": "\n    ",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% \n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [],
      "source": "import tensorflow as tf\nimport gym\nimport numpy as np\nfrom scipy import signal\n\nclass BatchHandler:\n    \n    def __init__(self,\n                 obs_space: gym.spaces.Box,\n                 action_space: gym.spaces.Discrete,\n                 batch_size,\n                 dtype\u003dtf.float32):\n        \n        self.action_space \u003d action_space\n        \n        # Convert dtype from tf to numpy\n        if dtype \u003d\u003d tf.float32:\n            dtype \u003d np.float32\n        elif dtype \u003d\u003d tf.float16:\n            dtype \u003d np.float16\n        \n        \n        self.b_obs \u003d np.zeros(\n            shape\u003d(batch_size, ) + obs_space.shape,\n            dtype\u003ddtype\n        )\n\n        self.b_act \u003d np.zeros(\n            shape\u003d(batch_size,),\n            dtype\u003ddtype\n        )\n        \n        self.b_act_logits \u003d np.zeros(\n            shape\u003d(batch_size, action_space),\n            dtype\u003ddtype\n        )\n        \n        self.b_rew \u003d np.zeros(\n            shape\u003d(batch_size,),\n            dtype\u003ddtype\n        )\n        \n        self.batch_size \u003d batch_size\n        self.counter \u003d 0\n    \n    def add(self, obs, action, action_logits, reward):\n        self.b_obs[self.counter] \u003d obs\n        self.b_act[self.counter] \u003d action\n        self.b_act_logits[self.counter] \u003d action_logits\n        self.b_rew[self.counter] \u003d reward\n        \n        if self.counter \u003d\u003d self.batch_size - 1:\n            self.counter \u003d 0\n            return True\n            \n        self.counter +\u003d 1\n        return False    \n\nclass PGPolicy(tf.keras.models.Model):\n    \n    def __init__(self, action_space: gym.spaces.Discrete, dtype\u003dtf.float32):\n        super().__init__(\u0027pg_policy\u0027)\n        self._dtype \u003d dtype\n        \n        self.h_1 \u003d tf.keras.layers.Dense(32, activation\u003d\u0027relu\u0027, dtype\u003dself._dtype)\n        self.h_2 \u003d tf.keras.layers.Dense(32, activation\u003d\u0027relu\u0027, dtype\u003dself._dtype)\n        self.h_3 \u003d tf.keras.layers.Dense(23, activation\u003d\u0027relu\u0027, dtype\u003dself._dtype)\n        \n        # Probabilties of each action\n        self.logits \u003d tf.keras.layers.Dense(action_space, activation\u003dNone, name\u003d\u0027policy_logits\u0027, dtype\u003dself._dtype)\n        \n        self.optimizer \u003d tf.keras.optimizers.RMSprop(lr\u003d0.0007) # TODO dytnamic\n        self.compile(\n                    optimizer\u003dself.optimizer,\n                    loss\u003d[self._loss_logits]\n                )\n\n    \n    def call(self, inputs):\n        x \u003d tf.convert_to_tensor(inputs, dtype\u003dself._dtype) \n        x \u003d self.h_1(x)\n        #x \u003d self.h_2(x)\n        #x \u003d self.h_3(x)\n        out \u003d self.logits(x)\n        return out\n        \n    def _loss_logits(self, actual_y, pred_y):\n        # http://inoryy.com/post/tensorflow2-deep-reinforcement-learning/\n        actions, advantages \u003d tf.split(actual_y, 2, axis\u003d-1)\n        \n        weighted_sparse_ce \u003d tf.keras.losses.SparseCategoricalCrossentropy(from_logits\u003dTrue)\n        actions \u003d tf.cast(actions, tf.int32)\n        policy_loss \u003d weighted_sparse_ce(actions, pred_y, sample_weight\u003dadvantages)\n        entropy_loss \u003d tf.keras.losses.categorical_crossentropy(pred_y, pred_y, from_logits\u003dTrue)\n        return policy_loss - 0.001 *entropy_loss\n    \n    def train(self, observations, actions, actions_logits, discounted_rewards):\n  \n        acts_and_advs \u003d np.concatenate([actions[:, None], discounted_rewards[:, None]], axis\u003d-1)\n     \n        losses \u003d self.train_on_batch(observations, [acts_and_advs])\n        print(losses)\n        return losses\n            \n            \n\nclass PGAgent:\n    \n    def __init__(self, \n                 obs_space: gym.spaces.Box,\n                 action_space: gym.spaces.Discrete,\n                 gamma\u003d0.99, # Discount factor\n                 batch_size\u003d1,\n                 dtype\u003dtf.float32):\n        \n        self.gamma \u003d gamma\n        \n        self.action_space \u003d action_space\n        \n        self.policy \u003d PGPolicy(\n            action_space\u003daction_space,\n            dtype\u003ddtype\n        )\n        \n        self.batch \u003d BatchHandler(\n            obs_space\u003dobs_space,\n            action_space\u003daction_space,\n            batch_size\u003dbatch_size,\n            dtype\u003ddtype\n        )\n        \n        self.last_observation \u003d None\n        self.last_action \u003d None\n        self.last_action_logits \u003d None\n        \n    def reset(self):\n        self.last_observation \u003d None\n        self.last_action \u003d None\n        self.last_action_logits \u003d None\n        self.batch.counter \u003d 0\n        \n    def predict(self, observation):\n        action_logits \u003d self.policy.predict(observation)\n        self.last_action_logits \u003d action_logits\n        \n        action_sample \u003d tf.squeeze(tf.random.categorical(action_logits, 1)).numpy()\n        \n        self.last_observation \u003d observation\n        self.last_action \u003d action_sample\n        \n        \n        return action_sample\n    \n    def observe(self, reward):\n        \n        is_full \u003d self.batch.add(\n            obs\u003dself.last_observation, \n            action\u003dself.last_action, \n            action_logits\u003dself.last_action_logits,\n            reward\u003dreward\n        )\n        \n        # Batch is full.\n        if is_full:\n            \n            #actions_and_logits \u003d np.concatenate([\n            #    self.batch.b_act[:, None], \n            #    self.batch.b_act_logits[:, None]\n            #], axis\u003d-1)\n            \n            observations \u003d self.batch.b_obs\n            actions \u003d self.batch.b_act\n            actions_logits \u003d self.batch.b_act_logits\n            rewards \u003d self.batch.b_rew\n            \n            losses \u003d self.policy.train(observations, actions, actions_logits, self._discounted_rewards(rewards))\n            return losses\n\n    def _discounted_rewards(self, episode_rewards):\n        \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n        discounted_r \u003d np.zeros_like(episode_rewards)\n        running_add \u003d 0\n        for t in reversed(range(0, episode_rewards.size)):\n            #if r[t] !\u003d 0: running_add \u003d 0 # reset the sum, since this was a game boundary (pong specific!)\n            running_add \u003d running_add * self.gamma + episode_rewards[t]\n            discounted_r[t] \u003d running_add\n            \n        discounted_r -\u003d discounted_r.mean()\n        discounted_r /- discounted_r.std()\n        \n        return discounted_r\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "-0.0013617369\n-0.016735207\n-0.07514664\n0.06602718\n0.06956212\n0.06020675\n-0.10450711\n-0.30911234\n-0.051262133\n-0.034170356\n0.34022716\n0.054224692\n0.021173744\n",
            "0.08969717\n0.100440025\n0.009390388\n-0.05487892\n0.12053683\n0.0030912957\n0.01026321\n-0.023374163\n-0.12600274\n0.056313083\n0.14557771\n-0.03539244\n0.017897598\n-0.05423072\n",
            "-0.31782568\n-0.06807476\n0.051415935\n0.016246816\n0.03580606\n0.040626023\n-0.053464696\n0.04489786\n-0.008301324\n0.119335845\n0.088634536\n0.008831598\n0.027109846\n0.10294019\n",
            "-0.07586465\n-0.023696706\n0.05657626\n-0.064632565\n0.059798557\n0.043465413\n-0.26420546\n-0.14404768\n-0.021309298\n0.13886853\n-0.045178823\n0.0030867024\n0.08863833\n0.009941473\n",
            "-0.22023854\n0.058307275\n-0.070489995\n0.024035871\n0.07700421\n0.052212834\n0.0023313025\n-0.008762917\n0.09426248\n-0.0984613\n0.030175935\n-0.20274293\n-0.25919795\n-0.3177393\n",
            "0.03745797\n0.20695809\n-0.033254694\n0.059724838\n0.009446939\n-0.2577966\n0.004890954\n-0.11574559\n-0.02174506\n0.02593891\n0.049329508\n-0.018100388\n0.054605585\n0.07056083\n",
            "0.059746914\n0.117221564\n0.018907797\n0.024857124\n-0.042271264\n0.17228884\n-0.084004916\n-0.17177933\n0.17292246\n0.08381512\n0.021321183\n0.00430225\n0.00018220543\n0.0027040215\n",
            "0.05075495\n0.09698168\n-0.15324634\n0.0942896\n0.0870156\n-0.069951504\n-0.09697999\n0.08170958\n-0.22456318\n0.0029062273\n0.087052174\n0.17591888\n0.024327245\n-0.03133252\n",
            "0.0026783277\n0.08985546\n0.064741224\n-0.07880917\n-0.058832113\n-0.012764416\n0.009136964\n-0.59761894\n-0.031231947\n-0.18823537\n0.04964509\n-0.24448639\n-0.06226343\n-0.12685911\n",
            "0.052446395\n0.0377094\n-0.07866098\n-0.17873001\n0.0010565592\n0.04214859\n0.05713207\n0.033654165\n-0.16464469\n0.0013863097\n-0.02070973\n0.007935869\n0.101244755\n0.06275647\n",
            "0.030116927\n0.006006962\n-0.008675747\n0.02114465\n-0.0122174835\n0.015252585\n-0.00055379415\n0.04482981\n0.043457113\n-0.066296116\n-0.17771983\n-0.21146065\n0.080548756\n0.1615158\n",
            "-0.016904363\n0.04277105\n0.07235752\n-0.18582827\n0.07525416\n-0.2778592\n-0.050306313\n-0.023226164\n0.02497555\n-0.1585193\n0.075717464\n0.101070486\n-0.06590581\n-0.016612858\n",
            "-0.45266086\n-0.1632986\n-0.020840999\n0.02515205\n-0.14099926\n0.0120318355\n0.20265773\n-0.011310195\n0.042927317\n-0.52934843\n0.043558262\n-0.046623643\n-0.051058136\n-0.09826946\n",
            "0.114137016\n0.17114633\n0.019602478\n-0.030663408\n-0.040339187\n0.057084933\n0.11536065\n-0.05269807\n-0.03264349\n0.08774726\n0.057930373\n-0.036902834\n-0.18983278\n0.009264206\n",
            "0.5998041\n0.19790645\n-0.013499712\n0.004033517\n-0.06765897\n0.04141081\n0.045337066\n0.13064554\n0.11950252\n-0.038858056\n-0.14733747\n0.229532\n0.05376699\n0.19009683\n",
            "0.11918931\n-0.011240421\n-0.07377258\n-0.2508964\n-0.05699294\n-0.068635225\n0.013058122\n0.08395817\n0.28606775\n-0.28514183\n0.08274411\n0.064187646\n0.033505075\n-0.16383739\n",
            "0.21949646\n-0.016791657\n-0.057633117\n0.040843338\n0.032698408\n-0.065639034\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "\nenv \u003d gym.make(\u0027CartPole-v0\u0027)\nagent \u003d PG(\n    obs_space\u003denv.observation_space,\n    action_space\u003denv.action_space.n,\n    batch_size\u003d32\n)\n\n\n\nfor e in range(300):\n    steps \u003d 0\n    terminal \u003d False\n    obs \u003d env.reset()\n    #agent.reset()\n    cum_loss \u003d 0\n    while not terminal:\n        action \u003d agent.predict(obs[None, :])\n        obs, reward, terminal, info \u003d env.step(action)\n        reward \u003d 0 if terminal else reward\n        \n        losses \u003d agent.observe(reward)\n        if losses is not None:\n            cum_loss +\u003d losses\n        steps +\u003d 1\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": true
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 352,
      "outputs": [],
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}